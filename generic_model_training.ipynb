{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from nn.lstm import SequenceLSTM\n",
    "from nn.utils import RMSLELoss\n",
    "from custom_dataset import SlidingWindowDataset\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import collections\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training variables\n",
    "\n",
    "DATASET = \"demand\"\n",
    "SCALE_DIVISOR = 1\n",
    "BATCH_SIZE = 1\n",
    "VALUE_COLUMN = \"y\"\n",
    "WINDOW_SIZE = 1\n",
    "LOSS_FUNCTION = \"mae\"\n",
    "#LOSS_FUNCTION = \"mse\"\n",
    "#LOSS_FUNCTION = \"rmsle\"\n",
    "\n",
    "LEARNING_RATE = 3e-4 # Chosen since Karpathy recommends 3e-4 for Adam\n",
    "#DATA_FORMAT = \"csv\"\n",
    "DATA_FORMAT = \"feather\"\n",
    "\n",
    "ROUNDS = 1\n",
    "FRACTION = 0.1\n",
    "\n",
    "MODEL_NAME = \"lr\" + str(LEARNING_RATE) + \"_mae\"\n",
    "\n",
    "device = None\n",
    "DISABLE_CUDA = False\n",
    "if not DISABLE_CUDA and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(4)\n",
    "np.random.seed(4)\n",
    "random.seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor of shape [n, m] -> Tensor of shape [m, n, 1]\n",
    "def shape(tensor: torch.Tensor):\n",
    "    t = tensor.permute(1, 0)\n",
    "    return t.view(t.size(0), t.size(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit: https://www.jessicayung.com/lstms-for-time-series-in-pytorch/\n",
    "class SequenceLSTM(nn.Module):\n",
    "    def __init__(self, input_size_is_num_features, hidden_size):\n",
    "        super(SequenceLSTM, self).__init__()\n",
    "        self.input_size = (\n",
    "            input_size_is_num_features  # For us it's 1 since each timestep is 1 number\n",
    "        )\n",
    "        self.hidden_size = hidden_size  # arbitrary\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size_is_num_features, hidden_size, 1\n",
    "        )  # one layer in LSTM\n",
    "        self.linear = nn.Linear(hidden_size, 1)  # Infer just 1 future step\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input of shape (in_a_batch, seq_len_is_timesteps_per_sequence, num_features_input_size)\n",
    "        input = shape(input)\n",
    "        batch_size = input.size(1)\n",
    "\n",
    "        # h0 = torch.randn(num_layers, in_a_batch, hidden_size_arbitrary)\n",
    "        # use .new_zeros to ensure it's on the same device (cpu/gpu)\n",
    "        h_t = input.new_zeros(\n",
    "            1, batch_size, self.hidden_size, dtype=torch.double\n",
    "        )  # 1 is num of layers\n",
    "        c_t = input.new_zeros(1, batch_size, self.hidden_size, dtype=torch.double)\n",
    "\n",
    "        lstm_out, (h_t, c_t) = self.lstm1(input, (h_t, c_t))\n",
    "\n",
    "        # Only take the output from the final timetep - lstm_out[-1]\n",
    "        y_pred = self.linear(lstm_out[-1].view(batch_size, -1))\n",
    "        return y_pred.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks-in-python\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Sliding dataset over an array with window size n, return n+1 as target.\"\"\"\n",
    "\n",
    "    def __init__(self, tensor, size):\n",
    "        self.data = tensor\n",
    "        self.window_size = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_size  # Number of sequences\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get single sequence by idx and split the last element away as target\n",
    "        return (\n",
    "            self.data[idx : idx + self.window_size],\n",
    "            self.data[(idx + self.window_size)],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Adjust path based on needs\n",
    "BASE_PATH = \"/content/drive/My Drive/Colab Notebooks/data/\" + DATASET\n",
    "\n",
    "with open(BASE_PATH + '/' + 'train.json') as json_file:\n",
    "    train_paths = [BASE_PATH + \"/train/\" + p for p in json.load(json_file)]\n",
    "\n",
    "with open(BASE_PATH + '/' + 'valid.json') as json_file:\n",
    "    valid_paths = [BASE_PATH + \"/valid/\" + p for p in json.load(json_file)]\n",
    "\n",
    "print(len(train_paths), \"training files\")\n",
    "print(len(valid_paths), \"validation files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/rmsle-loss-function/67281\n",
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = None\n",
    "if LOSS_FUNCTION == \"mse\":\n",
    "    loss_fn = nn.MSELoss()\n",
    "elif LOSS_FUNCTION == \"rmsle\":\n",
    "    loss_fn = RMSLELoss()\n",
    "elif LOSS_FUNCTION == \"mae\":\n",
    "    loss_fn = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only read the files of the validation dataset in one go\n",
    "valid_dfs = [pd.read_feather(p) for p in valid_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dss = [SlidingWindowDataset(torch.from_numpy(df[VALUE_COLUMN].to_numpy() / SCALE_DIVISOR).double().to(device=device), WINDOW_SIZE) for df in valid_dfs\n",
    "  if len(df) > WINDOW_SIZE\n",
    "]\n",
    "\n",
    "valid_ds = ConcatDataset(valid_dss)\n",
    "valid_dl = DataLoader(\n",
    "    valid_ds,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    batch_size=100\n",
    ")\n",
    "valid_input, valid_target = next(iter(valid_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(BASE_PATH + \"/params\"):\n",
    "    os.mkdir(BASE_PATH + \"/params\")\n",
    "\n",
    "if not os.path.isdir(BASE_PATH + \"/params/\" + MODEL_NAME):\n",
    "    os.mkdir(BASE_PATH + \"/params/\" + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MODEL_NAME)\n",
    "global_model = SequenceLSTM(1, 100).double()\n",
    "global_model.double().to(device=device)\n",
    "\n",
    "for t in range(ROUNDS):\n",
    "    print(\" ROUND {}\".format(t))\n",
    "    m = int(max(FRACTION * len(train_paths), 1))\n",
    "    print(m, \"partitions per round\")\n",
    "    selected_partitions = random.sample(train_paths, m)\n",
    "    total_n = 0\n",
    "    local_ns = []  # Store n_k here\n",
    "    partition_weights = [] # Store the final model of the partition here for averaging\n",
    "    training_losses = []\n",
    "    for k in range(len(selected_partitions)):\n",
    "        partition_df = None\n",
    "        try:\n",
    "            partition_df = pd.read_feather(selected_partitions[k])\n",
    "        except:\n",
    "            print(\"Failure to read df\")\n",
    "            continue\n",
    "    partition_ds = SlidingWindowDataset(torch.from_numpy(partition_df[VALUE_COLUMN].to_numpy() / SCALE_DIVISOR).double().to(device=device), WINDOW_SIZE)\n",
    "    sys.stdout.write(\".\")\n",
    "    sys.stdout.flush()\n",
    "    # Train one partition\n",
    "    local_model = copy.deepcopy(global_model)\n",
    "    optimizer = optim.Adam(local_model.parameters(), lr=LEARNING_RATE)\n",
    "    local_dl = DataLoader(partition_ds, batch_size=BATCH_SIZE)\n",
    "    local_n = 0\n",
    "    for inputs, targets in iter(local_dl):\n",
    "        local_n = local_n + len(inputs)\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            out = local_model(inputs)\n",
    "            loss = loss_fn(out, targets)\n",
    "            loss.backward()\n",
    "            training_losses.append(loss.item())\n",
    "            return loss\n",
    "      \n",
    "      optimizer.step(closure)\n",
    "\n",
    "    local_ns.append(local_n)\n",
    "    total_n = total_n + local_n\n",
    "    partition_weights.append(local_model.state_dict())\n",
    "\n",
    "  # Average the models\n",
    "    next_model = collections.OrderedDict()\n",
    "    for k in range(len(partition_weights)):\n",
    "        local_model = partition_weights[k]\n",
    "        for key in global_model.state_dict().keys():\n",
    "            if key in next_model:\n",
    "                next_model[key] += (local_ns[k] / total_n) * local_model[key]\n",
    "            else:\n",
    "                next_model[key] =  (local_ns[k] / total_n) * local_model[key]\n",
    "\n",
    "    global_model.load_state_dict(next_model)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = global_model(valid_input)\n",
    "        print(\"Valid loss MSE:\", nn.MSELoss()(pred * SCALE_DIVISOR, valid_target * SCALE_DIVISOR))\n",
    "        print(\"Valid loss RSMLSE:\", RMSLELoss()(pred * SCALE_DIVISOR, valid_target * SCALE_DIVISOR))\n",
    "        print(\"Valid loss MAE:\", nn.L1Loss()(pred * SCALE_DIVISOR, valid_target * SCALE_DIVISOR))\n",
    "\n",
    "        torch.save(global_model.state_dict(), \n",
    "                    BASE_PATH + \"/params/{}/{}_round{}.pt\"\n",
    "                    .format(MODEL_NAME, MODEL_NAME, t))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
